<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Third Attempt</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Machine Learning CipherText</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Project Overview</a>
</li>
<li>
  <a href="data-overview.html">Data</a>
</li>
<li>
  <a href="f-attempt.html">First Attempt</a>
</li>
<li>
  <a href="s-attempt.html">Second Attempt</a>
</li>
<li>
  <a href="t-attempt.html">Third Attempt</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Third Attempt</h1>

</div>


<div id="third-and-hopefully-final-attempt" class="section level1">
<h1>Third and (hopefully) Final Attempt</h1>
<p>After the abysmal results achieved during the past two attempts, I needed to reframe the problem and see if I could solve for some of the issues that I learned along the way…</p>
<div id="issues" class="section level2">
<h2>Issues:</h2>
<ul>
<li>Too many classes
<ul>
<li>The problem with a rotating cipher is that each substitution changes after a certain textual event (for this one it changes after every capital letter). This produces that each cipher string/line can be unique, even if the plain text is the same - just at a different position.</li>
</ul></li>
<li>Too much / Not Enough data
<ul>
<li>Many of the models which used the full data set to train error ed out or failed to complete. Given the limited resources available for me, I need to reduce the volume of data that I use. However, reducing the number of examples creates an accuracy issue in that there are not enough samples to train adequately. Therefore, a balance/compromise needed to be found.</li>
</ul></li>
<li>Textual Embedding are crucial but difficult to create
<ul>
<li>Because all the models require numeric representation of the text, it is crucial that I find as many meaningful measurements that can be applied to the text blocks.</li>
<li>The H2O NN’s provided variable importance when it generates a model. From this information, I was able to manually decipher the text, but also highlight the important features to include.</li>
<li>The custom embedding that I used previously were very limited (single integer representations) which limit the prediction abilities.</li>
</ul></li>
</ul>
</div>
<div id="possible-solution-create-a-language-translation-model" class="section level2">
<h2>Possible Solution: Create a language translation model</h2>
<ul>
<li>Languages contain a large amount of independent cases (e.g. typically there are only a few exact choices for each word in another language)
<ul>
<li>However, there are also occurrences of “one to many” or “many to one” word/phrase correlations between different languages. This would accommodate the changes in cipher substitution.</li>
</ul></li>
<li>If I focus on the most frequent words/phrases within each (plain/cipher) then it may be possible to “translate” some of the text in each line to the degree that the remaining encrypted text can be extrapolated.</li>
<li>There are pre-defined word embedding that can be used to increase the numeric data “behind the scenes”.
<ul>
<li>Although I specifically chose the Shakespeare text because of its resemblance to encrypted English, there may be enough common words to exact benefit from pre-made embedding.</li>
</ul></li>
</ul>
</div>
<div id="tensorflows-lstm-network" class="section level2">
<h2>Tensorflow’s LSTM Network</h2>
<p>I decided to use a different Neural Network structure as H2Os Forward Feed produced the most favorable results but did not perform to expectation. Therefore, this next process will use Tensorflow with Python’s Keras’s front end, training a Long term Shor term Memory Network. As for the data, I removed the cipher text padding and aligned the plain text string with the “true” cipher text corresponding string. I also took the opportunity to remove punctuation as it was not encrypted (and increased the repetitive cases).</p>
<p>Modified LSTM process from: <a href="https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/" class="uri">https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/</a>, Usman Malik, 2019 - FRENCH LANGUAGE TRANSLATION</p>
<p>Attempting to create a translation dictionary of plain text and cipher text. This example will be using known plain text/cipher text relationships - however a similar data set could be executed given the relative frequencies of each term in the text and matching those that are close. While the former method would likely reduce the accuracy of the “translation”, it may offer enough correct terms to predict corresponding text relationships using by measuring their similarities.</p>
<p>The first attempt used the entire, matched, data set (27k + lines). Unfortunately, the system timed out prior to completion.</p>
<p>However….</p>
<p><img src="a3ltsmfull.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Before the build timed out, it registered an accuracy rate of over 89% on the first epoch!</p>
<p>I reduced the size of the data set, knowing that it would also impact its final ability to successfully predict some of the text.</p>
<p><img src="a3ltsmdata.png" width="70%" style="display: block; margin: auto;" /></p>
<p>he reduced data set consisted of 9203 unique plain text words and almost triple that in the cipher text. The large increase indicates that the cipher text includes multiple substitutions for similar words.</p>
<p><img src="a3lstmlen.png" width="70%" style="display: block; margin: auto;" /></p>
<p>After the first few failed attempts at model completion, I discovered that this type of iteration performs better with string lengths of less than 50 characters. Therefore, I again modified the data set and re-ran the model.</p>
<p>This instance also used GloVe’s prebuilt 100-vector length, word embedding which was applied to the plain text examples (matched when available). Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. [pdf]</p>
</div>
</div>
<div id="gloves-word-embedding" class="section level1">
<h1>GloVe’s Word Embedding</h1>
<p><img src="a3ltsmemb.png" width="60%" style="display: block; margin: auto;" /></p>
<p>The 100 vector word embedding for a single plain text tokenized word. Using this information GREATLY increases the amount of numeric measurement that the network can use to fit the model.</p>
</div>
<div id="neural-network-configuration" class="section level1">
<h1>Neural Network Configuration</h1>
<p><img src="ciphermod3.png" width="100%" /></p>
<p>The map of the network shows two embedding layers and two instances of LSTM. I am also using the prescribed number of nodes (256). This shows the combination of the input layer (plain text) joining with the second input layer (cipher text) and flowing into a dense network layer.</p>
<p><img src="a3ltsmprog.png" width="100%" /></p>
<p>The model did take quite a while to cycle through the 10 training epochs, but it did complete. However, as a result of limiting the data set the accuracy rate achieved was only 75.6%. Not as good as the full-set model, but a marked improvement from the 0.8% accuracy rates from the previous attempts.</p>
</div>
<div id="prediction" class="section level1">
<h1>Prediction</h1>
<p>Now that the LSTM model has been trained, it is time to see if it can successfully “translate” plain text into the correct cipher text</p>
<p><img src="a3ltsmpred.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The prediction output did not match the plain text string at all. However, given the repeated word (ihd), I feel that the issue lies within my code and not with the abilities of the model.</p>
<div id="h2o-gradient-boost-machine-with-t" class="section level2">
<h2>H2O Gradient Boost Machine with T</h2>
<p>Given the possible success with the LSTM I decided to try H2O’s Gradient Boost Machine using their new word2vec embedding function. The example of using this functionality was provided by H2O’s website: <a href="https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.word2vec.craigslistjobtitles.R" class="uri">https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.word2vec.craigslistjobtitles.R</a> (sebhrusen, 2019).</p>
<p>I believe that some of the issues that I had with Tensorflows LSTM Network was a result of using the entire plain text/cipher text strings. Therefore, I modified the data set to capture only the first word of the plain text and filtered for specific Shakespeare characters. While this change will not identify specific lines of text, it will narrow down the section/block based on the speaker of the line. For this exercise I am using known text pairs so that I can determine the accuracy of the translation/decryption. However, if the pairs were unknown, the data set could be approximated by filtering for all capital plain and cipher words at the beginning of each line. Collapsing the plain text by only certain first-words does increase the repeated patterns that the machine can use for prediction. It also increases the variation of encrypted options so that searches can be performed on multiple cipher-words.</p>
</div>
</div>
<div id="modifying-the-data-set" class="section level1">
<h1>Modifying the Data set</h1>
<pre class="r"><code>library(utils)
library(stringr)
library(tidyr)
library(readr)
corp &lt;-read_csv(&quot;/Users/ckornafel/Desktop/MSDS692 Data Science Practicum I/corp.csv&quot;, col_types = cols())

#Removing punctuation
corp &lt;-as.data.frame(sapply(corp, function(x) as.character(gsub(&#39;[[:punct:]]+&#39;, &quot; &quot;,x))))

#Extracting the first two terms
corp$plain_fw &lt;- word(corp$plain, 1,2, sep = &quot; &quot;)

#Reordering the dataframe columns (putting response in front) and dropping plain text column
corp &lt;- corp[,c(3,2)]

names &lt;- c(&quot;KING HENRY&quot;, &quot;GLOUCESTER&quot;, &quot;HAMLET&quot;, &quot;BRUTUS&quot;, &quot;QUEEN MARGARET&quot;, &quot;MARK ANTHONY&quot;, &quot;PORTIA&quot;, &quot;FALSTAFF&quot;, &quot;DUKE VINCENTIO&quot;, &quot;KING LEAR&quot;,
           &quot;PROSPERO&quot;, &quot;TITUS ANDRONICUS&quot;, &quot;IMOGEN&quot;, &quot;ROSALIND&quot;, &quot;MACBETH&quot;, &quot;HELENA&quot;, &quot;CORIOLANUS&quot;, &quot;BIRON&quot;, &quot;PRINCE HENRY&quot;)

#Filtering for the parts above
corp_names &lt;- subset(corp, plain_fw %in% names) #sorting by the reduced plaintext terms

head(corp_names)</code></pre>
<pre><code>##      plain_fw                                                true_ciphertext
## 4  KING HENRY      JTRV GPRHX TA  Iglpb mab  xm xyjtlp mdwp qdeitltrv qlrar 
## 14 KING HENRY AHYK WDYVO HH  Edc si wyfl qqayvgf yi rxsess edc iibbaqt mpbi 
## 15 KING HENRY             VMDF SIDQK ML  Ssi Tydp ee Oskfwei he hxrnscetxtc 
## 17 KING HENRY        VMDF SIDQK ML  Lavsyvi jgp Ipqw su Etjt  yyh tkoiis esd
## 19 KING HENRY  OXMR LTMDD XU  Tr rqlhbd npessii ngv rgtpsqpr mgpvt ssio kld 
## 21 KING HENRY     OXMR LTMDD XU  Aj jgtw ongrv Opvrx e thhoi  jgp thhesdddw</code></pre>
<p>As the above example shows, the data set was reduced to a single plain text class word and the remaining cipher text string was included. The later will be processed through the word2vec function to identify word embedding</p>
<p>The GBM model was created using the base parameters and trained using a 80/20 split</p>
<pre class="r"><code>#Tokenize the ciphertext
c_words &lt;- tokenize(cipher_corp$cipher)

#Use the H2O word to vector function for word embeddings
w2v_model &lt;- h2o.word2vec(c_words, sent_sample_rate = 0, epochs = 10) 

#Transforming the prediction into vectors to use in the GMB model
cipher_vecs &lt;- h2o.transform(w2v_model, c_words, aggregate_method = &quot;AVERAGE&quot;)


valid_cipher &lt;- ! is.na(cipher_vecs$C1) #Checking for valid characters
data &lt;- h2o.cbind(cipher_corp[valid_cipher, &quot;plain&quot;], cipher_vecs[valid_cipher, ])
data.split &lt;- h2o.splitFrame(data, ratios = 0.8) #splitting the set into train and test

#Creating the GBM model 
gbm &lt;- h2o.gbm(x = names(cipher_vecs), y = &quot;plain&quot;,
                     training_frame = data.split[[1]], validation_frame = data.split[[2]])

#Make Predictions using a line from Duke V
deciphert(&quot;HKJP AXMNIDSTS  Cx exhdykjg ned ftat xay  jgpr&quot;, w2v_model, gbm)</code></pre>
</div>
<div id="general-default-gbm-model-parameters" class="section level1">
<h1>General (default) GBM Model Parameters</h1>
<p><img src="a3h2ogbm.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="gbm-model-performance" class="section level1">
<h1>GBM Model Performance</h1>
<p><img src="a3h2ogbmacc.png" width="60%" style="display: block; margin: auto;" /></p>
<p>With the updated data set and the vectorized cipher tokens, the GBM model performed very well with a low MSE rate. This is the best performance noted to date.</p>
</div>
<div id="testing-the-prediction-capability" class="section level1">
<h1>Testing the Prediction Capability</h1>
<p>I randomly selected a cipher string that corresponded with Duke Vincentio and attempted to see if the GBM model could successfully identify the speaker/part of the Shakespeare Play</p>
<p><img src="a3h2opred.png" width="100%" /></p>
<p>As the output shows, the model did identify the correct plain text speaker/part of the play corresponding to the cipher text entered. Additionally, the model provided the top six possible matches with a numeric indication of their likelihood.</p>
</div>
<div id="h2o-conclusion" class="section level1">
<h1>H2O Conclusion</h1>
<p>The final implementation of H2O GBM using the word2vec function required very little code yet produced the best results from this project. I do realize; however, that most of the benefit was derived from the greatly reduced data set (focusing only on certain first plain text words). This leads me to believe that multi-classification, using neural networks is a potential tool for cipher text encryption. However, it would require more computational resources than an average college student has access to.</p>
<div id="overall-thoughts" class="section level2">
<h2>Overall Thoughts</h2>
<p>Many of the machine learning models were not able to successfully identify patterns within the encrypted text, given the resources available. However, there were some “glimmers” of hope with the use of neural networks. I believe that increasing the size of the network (adding more hidden layers) and expanding the data used could result in better performance. Building a viable fully homomorphic encryption scheme, in which the machine recognizes encoded data and is able to process it without decryption would require large, complex neural networks.</p>
<p>It was shown that the type of data is very important when it comes to building a successful machine learning model. I found that modifying the text into a custom embedding scheme took a lot of effort but yielded very little results. Additionally, the fact that I manually uncovered the type of cipher being used for the challenge well before achieving any form of automated success, demonstrates that current (inexpensive) machine learning techniques are not cost effective for this type of project. However, the process did produce useful information, e.g. variable importance, which did assist with determining which character types I should focus on.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
