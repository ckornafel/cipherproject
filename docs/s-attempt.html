<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Second Attempt</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Machine Learning CipherText</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Overview</a>
</li>
<li>
  <a href="data-overview.html">Data</a>
</li>
<li>
  <a href="f-attempt.html">First Attempt</a>
</li>
<li>
  <a href="s-attempt.html">Second Attempt</a>
</li>
<li>
  <a href="t-attempt.html">Third Attempt</a>
</li>
<li>
  <a href="conclusion.html">Conclusion</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Second Attempt</h1>

</div>


<div id="second-attempt" class="section level2">
<h2>Second Attempt</h2>
<p>After the abysmal performance achieved using the first modified data sets (attribute counts, plain text-only training set), I decided to take a different approach. I learned that I needed fewer classes than each row and I should attempt to include cipher text examples in the training set. Another issue that I identified was the random component of the cipher text character padding. In an effort to remedy some of these issues, I began to focus on the individual words within each string as opposed to viewing the entire string.</p>
<p>While I was experimenting with the original data, I was able to determine the padding scheme used for the level 1 encryption. Each cipher text line split the number of random characters pre and post the true encrypted string (i.e. the actual Shakespeare text was located in the middle of the encrypted string). I made the decision to remove the padding so that I could expose the actual encryption pattern which would hopefully improve prediction accuracy. Additionally, I did notice that the encryption scheme used was a rotating poly alphabetic substitution which changed after every capital letter (similar to a Caesar Shift cipher). This highlights that manual effort achieved more than using the machine learning techniques already. However, I attempted to recreate the information that I used in breaking the encryption method.</p>
<p>The new approach: * Listed individual words of plain text with their corresponding encrypted text + This processed greatly reduced the amount of classes due to the repetition of words throughout the Shakespeare text. However, there were still over 1000 combination of alpha/punctuation so I did cap the training size to 1000. * Words were separated via space characters (as identified in the data exploration section). + Since both plain and cipher texts used the same spacing scheme, it produced equal number of word pairs. * All punctuation was included since it these characters represented similarities between each pair. * To further reduce file size, the total number of occurrences for each plain/cipher pair within the data sets was added. * The longest word/punctuation combination was 48 characters long. + This produced 48 individual columns for each word - shorter words had 0’s filled in for the missing characters (I was adding my own padding!) * The majority of the columns listed individual characters were of chr-typed that would be one-hot encoded prior to training the various models. * The models used include: Decision Tree, SVM, KNN, and H2O’s Forward Feed Neural Network.</p>
<p>The complete Python code can be found: <a href="https://github.com/ckornafel/cipherproject/blob/master/code/CipherOneHotLetter.py" class="uri">https://github.com/ckornafel/cipherproject/blob/master/code/CipherOneHotLetter.py</a></p>
</div>
<div id="data-set-sample" class="section level1">
<h1>Data set Sample</h1>
<pre class="python"><code>import pandas as pd
ltr = pd.read_csv(&#39;/Users/ckornafel/Desktop/MSDS692 Data Science Practicum I/cipher_ltr.csv&#39;)</code></pre>
<pre><code>## sys:1: DtypeWarning: Columns (23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48) have mixed types. Specify dtype option on import or set low_memory=False.</code></pre>
<pre class="python"><code>ltr = ltr.fillna(0) #Filling in the missing values with a 0

ltr.head()</code></pre>
<pre><code>##                                           plainwd  occur  p_wd_len  ... 44 45 46
## 0                                         --give,      1         7  ...  0  0  0
## 1  --Handkerchief--confessions--handkerchief!--To      1        46  ...  -  X  e
## 2                                          --Is&#39;t      1         6  ...  0  0  0
## 3                                          --what      1         6  ...  0  0  0
## 4                                        -groves,      1         8  ...  0  0  0
## 
## [5 rows x 49 columns]</code></pre>
</div>
<div id="one-hot-encoding-the-sample-set" class="section level1">
<h1>One-Hot Encoding the Sample Set</h1>
<pre class="python"><code>SAMPLE_SIZE = 1000
ltr_samp = ltr.sample(n=SAMPLE_SIZE) #Obtaining a random sample of 1000 items

p_target_samp = ltr_samp[&#39;plainwd&#39;].astype(&#39;category&#39;) #coding the response variable
ltr_samp = ltr_samp.drop(&#39;plainwd&#39;, axis = 1) #Removing response from feature set

one_hot_samp = pd.get_dummies(ltr_samp) #One hot encoding the features

one_hot_samp.head()</code></pre>
<pre><code>##        occur  p_wd_len  1_&#39;  1_A  1_B  1_C  ...  41_0  42_0  43_0  44_0  45_0  46_0
## 28803      1         5    0    0    0    0  ...     1     1     1     1     1     1
## 34608      1         8    0    0    0    0  ...     1     1     1     1     1     1
## 20622      1         7    0    0    0    0  ...     1     1     1     1     1     1
## 42770      1         8    0    0    0    0  ...     1     1     1     1     1     1
## 16891      1         8    0    0    0    0  ...     1     1     1     1     1     1
## 
## [5 rows x 559 columns]</code></pre>
<p>As the example shows above, each of the character variables have been expanded based on their values to 598 columns. Regardless of the greater amount of information, these variables were still single interger (actually boolean) values</p>
</div>
<div id="breaking-the-set-into-traintest-and-scaling-the-values" class="section level1">
<h1>Breaking the set into Train/Test and Scaling the Values</h1>
<pre class="python"><code>from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split

#Identifying feature and target
features_samp = one_hot_samp
target_samp = p_target_samp

#Sample Size Data
x_samp_train, x_samp_test, y_samp_train, y_samp_test = train_test_split(features_samp, target_samp, random_state = 0)

#Scaling the datasets
scaler1 = StandardScaler()
scaler1.fit(x_samp_train)</code></pre>
<pre><code>## StandardScaler(copy=True, with_mean=True, with_std=True)</code></pre>
<pre class="python"><code>x_samp_train = scaler1.transform(x_samp_train)
x_samp_test = scaler1.transform(x_samp_test)</code></pre>
<p><img src="a2scaler.png" width="50%" style="display: block; margin: auto;" /></p>
<p>While not as sophisticated as word-type embedding, the one-hot encoding of scaled letter frequencies and positions does resemble a corpus-type format. My hope is that these vector values can provide enough information for the ML model to determine the cipher.</p>
<div id="decision-tree" class="section level2">
<h2>Decision Tree</h2>
<p>After training/testing a decision tree with a max depth of 100, using the full data set, I found that it took a lot of time to process (too much for RMarkdown) and did not produce any viable results. For this example, I am using the reduced sample set and allowing the model to expand to whatever depth it needs to create pure leaves.</p>
<pre class="python"><code>#Decision Tree
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import classification_report

##Reducing the dataset size so I can increase depth
#Creating the model without max depth value - this will fully extend each leaf
s_tree_mod = DecisionTreeClassifier()
s_tree_mod.fit(x_samp_train, y_samp_train)</code></pre>
<pre><code>## DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None,
##                        max_features=None, max_leaf_nodes=None,
##                        min_impurity_decrease=0.0, min_impurity_split=None,
##                        min_samples_leaf=1, min_samples_split=2,
##                        min_weight_fraction_leaf=0.0, presort=False,
##                        random_state=None, splitter=&#39;best&#39;)</code></pre>
<pre class="python"><code>s_tree_pred = s_tree_mod.predict(x_samp_test)

#Classification Report
s_tree_cr = classification_report(y_samp_test, s_tree_pred, output_dict=True)</code></pre>
<pre><code>## /Users/ckornafel/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
##   &#39;precision&#39;, &#39;predicted&#39;, average, warn_for)
## /Users/ckornafel/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.
##   &#39;recall&#39;, &#39;true&#39;, average, warn_for)</code></pre>
<pre class="python"><code>acc = s_tree_cr[&quot;accuracy&quot;]
print( &quot;Accuracy of Decision Tree: &quot;, acc)</code></pre>
<pre><code>## Accuracy of Decision Tree:  0.0</code></pre>
<pre class="python"><code>print( &quot;Precision of Decision Tree: &quot;, s_tree_cr[&quot;macro avg&quot;][&quot;precision&quot;])</code></pre>
<pre><code>## Precision of Decision Tree:  0.0</code></pre>
<p>The decision tree produced 0.4% accuracy and a precision of 0.25% precision. So this model did not produce accurate or repeatable predictions. However, these scores are marginally better than the previous attempt. Perhaps other models will yield better results.</p>
</div>
<div id="knn" class="section level2">
<h2>KNN</h2>
<p>Using KNN is popular for recommender systems that group similar items together based on their attributes. Hopefully, this model can find enough similarities within the data for better predictions than the decision tree.</p>
<pre class="python"><code>#KNN
from sklearn.neighbors import KNeighborsClassifier 
import numpy as np

#Finding the K with the lowest error
error = []
for i in range(1,50):
    knn = KNeighborsClassifier(n_neighbors = i)
    knn.fit(x_samp_train, y_samp_train)
    pred_i = knn.predict(x_samp_test)
    error.append(np.mean(pred_i != y_samp_test))</code></pre>
<p>For this model, I decided to run a neighbor search to see if I could find possible tuned parameters to construct the first KNN model. Finding the “perfect” number of neighbors is an art form.</p>
</div>
</div>
<div id="error-rates" class="section level1">
<h1>Error Rates</h1>
<p><img src="a2knnerror.png" width="40%" style="display: block; margin: auto;" /></p>
<p>The KNN error provided a few models that had a slightly less error rate than the other processes, but overall poor results. Perhaps, given the large number of individual dependent variables in the data sets, I should have increased the number of possible N’s. However, I went ahead and attempted some predictions based on the best 1-50 N model.</p>
</div>
<div id="predictions" class="section level1">
<h1>Predictions</h1>
<pre class="python"><code>#KNN
from sklearn.neighbors import KNeighborsClassifier 
NEIGHBORS = 100
s_knn_mod = KNeighborsClassifier(n_neighbors = NEIGHBORS)
s_knn_mod.fit(x_samp_train, y_samp_train)</code></pre>
<pre><code>## KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
##                      metric_params=None, n_jobs=None, n_neighbors=100, p=2,
##                      weights=&#39;uniform&#39;)</code></pre>
<pre class="python"><code>#Determining Accuracy
s_knn_mod_accuracy = s_knn_mod.score(x_samp_test,y_samp_test)
#print(&quot;KNN Accuracy: &quot;, s_knn_mod_accuracy)

#Predict
s_knn_pred = s_knn_mod.predict(x_samp_test)

knn_cr = classification_report(y_samp_test, s_knn_pred, output_dict=True)</code></pre>
<p><img src="a2knnacc.png" width="60%" style="display: block; margin: auto;" /></p>
<p>The accuracy did improve from the Decision Tree model, from 0.4% to 0.8% but the precision of the predictions dropped considerable. This would indicate that the improved accuracy (if one could call it improved) would not be consistently produced. The F1 score indicates that the true positives were very low in the prediction results so it appears that the model was better at predicting the negative cases. The likely source would be the high number of cases within the testing frame.</p>
</div>
<div id="knn-conclusion" class="section level1">
<h1>KNN Conclusion</h1>
<p>Again, using a random sample of the data set, I feel, is a drawback when using this model. Using a reduced sample allows the model to fit within a acceptable period of time (before the IDE times out) and it reduces the overall classes; however, it limits the available examples that could help precision and overall accuracy. The accuracy score was better than expected at this stage (although still horrible) but the corresponding low Precision and F1-Score reduce the overall improvement since it implies that lucky chance played a part.</p>
<div id="svm" class="section level2">
<h2>SVM</h2>
<p>The next model that I used is the support vector machine model which I also used with the previous data set. The repeated measurement could highlight if the modifications I performed on the train/test sets added benefit.</p>
<p>I attempted two kernel shapes with the data, linear and ovo. The One vs One (ovo) kernel is specific for multiclass classification which trains data based on the number of classes available.</p>
<p>The first linear SVM model performed as well as the previous one:</p>
<p><img src="a2svmlinacc.png" width="30%" style="display: block; margin: auto;" /></p>
<p>However, the One vs. One model showed a little promise:</p>
<p><img src="a2svmovoacc.png" width="30%" style="display: block; margin: auto;" /></p>
<p>Perhaps with some fine tuning the model would improve its performance</p>
<p>A grid search was performed using the kernels: linear, rbf, poly, and ovo with C values ranging from 1 to 10. However, the infrequent classes (there was a class with only one member) created issues with cross validating more than cv = 2.</p>
<pre class="python"><code>svc= SVC()
param = {&#39;kernel&#39;:(&#39;linear&#39;, &#39;rbf&#39;, &#39;poly&#39;), &#39;C&#39;: [1,10]}
grid = GridSearchCV(svc,param, cv=2)</code></pre>
<p>Unfortunately, the limited grid search showed that the linear kernel and a C value of 1 was the best fit. Since the OvO kernel already yielded the “best” results, I used that model for prediction.</p>
<p><img src="a2svmovopred.png" width="40%" style="display: block; margin: auto;" /> We can see that the accuracy remained the same with predictions. However, the precision and F1-Scores were pretty bad.</p>
</div>
<div id="svm-conclusion" class="section level2">
<h2>SVM Conclusion</h2>
<p>I feel that using a reduced sample data set also played a role in the poor performance. Perhaps if I manipulated the data set to include only the most frequent cases it would have performed better. The SVM OvO model did perform better than the previous attempt at using SVM, which would indicate that the kernel shape is important for this type of fit.</p>
</div>
<div id="h2o-forward-feed-neural-network" class="section level2">
<h2>H2O Forward Feed Neural Network</h2>
<p>I used the H2O NN with the previous data set and only had to reduce the data size to accommodate its 1000 case maximum. Therefore, it was able to utilize more data in determining and recognizing text patterns. For this attempt (focusing on words instead of the entire string), the number of cases has already been reduced. This is further reduced to create the train/validation sets - so no data is “technically” omitted. Additionally, NN’s require a lot of training data to generate results, so this adjustment should yield better performance. <img src="a2h2odataload.png" width="70%" style="display: block; margin: auto;" /></p>
<p>As we can see, the data set that was loaded into H2O was successfully parsed and contained 1501 rows with 49 columns. At this stage, the V1.. variables are stored as factors and will automatically be one-hot-encoded.</p>
<p>The H2O model constructed:</p>
<p><img src="a2h2omod1.png" width="40%" style="display: block; margin: auto;" /> The model was constructed with a small hidden layer to ensure completion and reduce running time, as well as using only 10 epochs.</p>
<p>However, the results were very similar to the previous attempt</p>
<p><img src="a2h2omod1res.png" width="30%" style="display: block; margin: auto;" /></p>
<p>This model was then cross-fold validated in an attempt to improve its performance. However, the number of cross-folds had to remain within the number of available cases and was set to nfolds = 3. This lower number was not expected to greatly improve results - which came true</p>
<p><img src="a2h2ocvmse.png" width="30%" style="display: block; margin: auto;" /> The cross-validation only yielded a reduction of 0.0012 in MSE. Therefore, I decided to perform a grid search to discover any parameter adjustments that could help improve results.</p>
<p><img src="a2h2ogridparam.png" width="50%" style="display: block; margin: auto;" /></p>
<p>For the grid search, I used multiple hidden layers (although still on the small side to accommodate computer resources). I also introduced several L1 values which helps improve generalization (given the low frequency of multiple cases).</p>
<p><img src="a2h2ogridout.png" width="100%" /></p>
<p>The results of the grid search yielded models which continually increased its logloss values. As a result, I focused only on the first model with the lowest logloss.</p>
<p><img src="a2h2ogridlogloss.png" width="40%" style="display: block; margin: auto;" /> We can see from the above plot that the performance of the “best” model increased for every epoch prior to 40. Given that I was only using 10 epochs for the initial model, it could indicate that this value needs increasing</p>
<p>The MSE did not show improvement for any of the possible models from the grid search. Therefore, I used the web interface to develop a more complex model (using many of the defaults). One of the benefits of using the web portal is that the models build much faster and with fewer computer resources. This allowed me to increase the size of the network</p>
<p><img src="a2h2owebmod.png" width="100%" /></p>
<p>As the above model shows, I was able to increase the hidden layer to 200, 200 nodes.</p>
<p><img src="a2h2owebper.png" width="60%" style="display: block; margin: auto;" /></p>
<p>The web-created model showed considerable improvement from the smaller network. This infers that large hidden layers are most likely necessary for this type of endeavor.</p>
</div>
<div id="h2o-conclusion" class="section level2">
<h2>H2O Conclusion</h2>
<p>Again, the neural network showed the most promise with this task when compared with the other models used in this iteration of the project. However, it was necessary to use the web portal to expand the neural network and capitalize on the power of hidden layers. I still feel that the lack of samples did have a negative impact on the overall performance.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
